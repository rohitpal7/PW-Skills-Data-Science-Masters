{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7355470-6a60-49d0-b2b5-135203d156e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1. What is Gradient Boosting Regression?\n",
    "#Ans.\n",
    "#Gradient Boosting Regression is a machine learning technique used for regression problems, where the goal is to predict a continuous target variable. \n",
    "#It is a type of boosting algorithm that combines multiple weak regression models to create a strong model that can make accurate predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8070aad7-c629-4c59-a547-96712db2a86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared.\n",
    "#Ans.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the dataset\n",
    "x = np.linspace(0, 2*np.pi, 50)\n",
    "y = np.sin(x) + np.random.normal(0, 0.1, size=50)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "split = int(0.8 * len(x))\n",
    "x_train, y_train = x[:split], y[:split]\n",
    "x_test, y_test = x[split:], y[split:]\n",
    "\n",
    "# Define the gradient boosting regression model\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.estimators = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Initialize the prediction with the mean of the target variable\n",
    "        prediction = np.mean(y) * np.ones(len(y))\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            # Compute the residuals as the difference between the true target values and the predictions\n",
    "            residuals = y - prediction\n",
    "            \n",
    "            # Train a decision tree on the residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            \n",
    "            # Update the prediction by adding the predictions of the new tree, scaled by the learning rate\n",
    "            prediction += self.learning_rate * tree.predict(X)\n",
    "            \n",
    "            # Add the new tree to the list of estimators\n",
    "            self.estimators.append(tree)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # Initialize the prediction with the mean of the target variable\n",
    "        prediction = np.mean(y_train) * np.ones(len(X))\n",
    "        \n",
    "        # Add the predictions of each tree to the current prediction\n",
    "        for tree in self.estimators:\n",
    "            prediction += self.learning_rate * tree.predict(X)\n",
    "            \n",
    "        return prediction\n",
    "\n",
    "# Define a decision tree regression model as a weak learner\n",
    "class DecisionTreeRegressor:\n",
    "    def __init__(self, max_depth=1):\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Compute the best split for the current node\n",
    "        best_split = self.get_best_split(X, y)\n",
    "        \n",
    "        # If no split is possible, set the prediction as the mean of the target variable\n",
    "        if best_split is None:\n",
    "            self.prediction = np.mean(y)\n",
    "            return\n",
    "        \n",
    "        # Create the left and right sub-trees\n",
    "        left_idx = X[:, best_split[0]] <= best_split[1]\n",
    "        right_idx = X[:, best_split[0]] > best_split[1]\n",
    "        self.left = DecisionTreeRegressor(max_depth=self.max_depth - 1)\n",
    "        self.right = DecisionTreeRegressor(max_depth=self.max_depth - 1)\n",
    "        \n",
    "        # Recursively fit the sub-trees\n",
    "        self.left.fit(X[left_idx], y[left_idx])\n",
    "        self.right.fit(X[right_idx], y[right_idx])\n",
    "        \n",
    "    def predict(self, X):\n",
    "        if hasattr(self, 'prediction'):\n",
    "            return self.prediction\n",
    "        \n",
    "        left_idx = X[:, self.split_feature] <= self.split_value\n",
    "        right_idx = X[:, self.split_feature] > self.split_value\n",
    "        prediction = np.zeros(len(X))\n",
    "        prediction[left_idx] = self.left.predict(X[left_idx])\n",
    "        prediction[right_idx] = self.right.predict(X[right_idx])\n",
    "        return prediction\n",
    "    \n",
    "    def get_best_split(self, X, y):\n",
    "        best_loss = np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4089e341-d061-4564-a1cb-664c6ee3b3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters\n",
    "#Ans.\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Define the dataset\n",
    "x = np.linspace(0, 2*np.pi, 50)\n",
    "y = np.sin(x) + np.random.normal(0, 0.1, size=50)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "split = int(0.8 * len(x))\n",
    "x_train, y_train = x[:split], y[:split]\n",
    "x_test, y_test = x[split:], y[split:]\n",
    "\n",
    "# Define the gradient boosting regression model\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.estimators = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Initialize the prediction with the mean of the target variable\n",
    "        prediction = np.mean(y) * np.ones(len(y))\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            # Compute the residuals as the difference between the true target values and the predictions\n",
    "            residuals = y - prediction\n",
    "            \n",
    "            # Train a decision tree on the residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            \n",
    "            # Update the prediction by adding the predictions of the new tree, scaled by the learning rate\n",
    "            prediction += self.learning_rate * tree.predict(X)\n",
    "            \n",
    "            # Add the new tree to the list of estimators\n",
    "            self.estimators.append(tree)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # Initialize the prediction with the mean of the target variable\n",
    "        prediction = np.mean(y_train) * np.ones(len(X))\n",
    "        \n",
    "        # Add the predictions of each tree to the current prediction\n",
    "        for tree in self.estimators:\n",
    "            prediction += self.learning_rate * tree.predict(X)\n",
    "            \n",
    "        return prediction\n",
    "\n",
    "# Define a grid of hyperparameters to search over\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'max_depth': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Perform a grid search over the hyperparameters\n",
    "model = GradientBoostingRegressor()\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(x_train.reshape(-1, 1), y_train)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding performance on the test set\n",
    "print('Best hyperparameters:', grid_search.best_params_)\n",
    "y_pred = grid_search.predict(x_test.reshape(-1, 1))\n",
    "print('MSE:', mean_squared_error(y_test, y_pred))\n",
    "print('R^2:', r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80deb155-713b-4d17-bb1b-271efa3eee76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4. What is a weak learner in Gradient Boosting?\n",
    "#Ans.\n",
    "#In Gradient Boosting, a weak learner is a model that performs only slightly better than random guessing on a given problem. \n",
    "#In other words, a weak learner is a model that has low predictive power, but is still able to capture some patterns in the data. \n",
    "#Examples of weak learners include decision trees with low depth, linear regression models, and simple neural networks with few hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a2a9ebd-9c38-416f-83b2-1718b22e86ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "#Ans.\n",
    "#The intuition behind the Gradient Boosting algorithm is to build a strong predictive model by combining several weak models in a sequential manner. \n",
    "#At each iteration of the algorithm, a new weak model is trained to correct the errors made by the previous models. \n",
    "#The algorithm does this by fitting the new model to the residuals (the difference between the true values and the predicted values) of the previous models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6b3962-6357-4def-9443-304662dcace2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "#Ans.\n",
    "#The Gradient Boosting algorithm builds an ensemble of weak learners in a sequential manner. At each iteration, a new weak learner is added to the ensemble to correct the errors made by the previous learners.\n",
    "#The algorithm starts by fitting a simple model, typically a decision tree with low depth, to the data. \n",
    "#This model is called the base learner. The base learner makes predictions on the training data, and the difference between the predicted values and the true values is calculated. This difference is called the residual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ff56bc-6a7e-4871-a80b-0d70061c22d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
    "#Ans.\n",
    "#The mathematical intuition behind the Gradient Boosting algorithm involves the following steps: \n",
    "#Initialize the ensemble, Calculate the residuals, Train the next learner, Update the ensemble, Repeat, Make predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
