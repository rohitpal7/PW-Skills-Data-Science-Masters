{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cba92b-6134-4a94-b3dc-77c6bc14f7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1. How does bagging reduce overfitting in decision trees?\n",
    "#Ans.\n",
    "#Bagging (Bootstrap Aggregating) is a technique that can help reduce overfitting in decision trees. Overfitting occurs when a model fits too closely to the training data and does not generalize well to new, unseen data. \n",
    "#Bagging helps to reduce overfitting in decision trees by creating multiple trees from different subsamples of the original training data, with replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945d4338-b5e0-4f44-a27d-24f637da5ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2. What are the advantages and disadvantages of using different types of base learners in bagging?\n",
    "#Ans.\n",
    "#Bagging (Bootstrap Aggregating) is a technique that can be used with a variety of base learners, including decision trees, neural networks, and support vector machines, among others. \n",
    "\n",
    "#Advantages of using decision trees as base learners in bagging include:Easy to understand and interpret, Robust to noise, Handle both categorical and numerical data\n",
    "#Disadvantages of using decision trees as base learners in bagging include:Prone to overfitting, Limited predictive power\n",
    "\n",
    "#Advantages of using neural networks as base learners in bagging include:High predictive power, Robust to noise\n",
    "#Disadvantages of using neural networks as base learners in bagging include: Computationally expensive, Black box model\n",
    "\n",
    "#Advantages of using support vector machines (SVMs) as base learners in bagging include: High predictive power, Robust to overfitting\n",
    "#Disadvantages of using SVMs as base learners in bagging include: Computationally expensive, Limited scalability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8818a59a-fedb-4589-bad5-2ec9c07431eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3. How does the choice of base learner affect the bias-variance tradeoff in bagging?\n",
    "#Ans.\n",
    "#The choice of base learner in bagging can have a significant impact on the bias-variance tradeoff.\n",
    "#the choice of base learner in bagging should be made based on the complexity of the problem being solved, the size of the dataset, and the desired trade-off between bias and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df8011f-e0cd-48f5-a0da-ee8a73fa611a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4. Can bagging be used for both classification and regression tasks? How does it differ in each case?\n",
    "#Ans.\n",
    "#Yes, bagging can be used for both classification and regression tasks.\n",
    "#Another difference is the evaluation metric used to measure the performance of the bagging ensemble. \n",
    "#In classification, metrics such as accuracy, precision, recall, and F1-score are commonly used, while in regression, metrics such as mean squared error (MSE) or mean absolute error (MAE) are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f16a91c-36f0-453f-955f-9337542260df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5. What is the role of ensemble size in bagging? How many models should be included in the ensemble?\n",
    "#Ans.\n",
    "#The ensemble size is a hyperparameter that controls the number of base models (i.e., decision trees) included in the bagging ensemble. \n",
    "#Increasing the ensemble size can lead to better performance, up to a certain point, after which further increasing the ensemble size may not improve the performance significantly and may even lead to diminishing returns.\n",
    "#It is worth noting that the optimal ensemble size may vary depending on the problem and dataset, and it is often determined empirically through experimentation and tuning of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941e7fc6-bbac-44dd-8585-0ef0ad50cbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6. Can you provide an example of a real-world application of bagging in machine learning?\n",
    "#Ans.\n",
    "#One real-world application of bagging in machine learning is in the field of credit risk assessment. Credit risk assessment involves predicting the likelihood of a borrower defaulting on a loan based on their credit history and other financial and personal information.\n",
    "#For example, the German Credit Data dataset is a popular benchmark dataset used for credit risk assessment. \n",
    "#In this dataset, bagging has been shown to improve the accuracy of the credit risk models compared to a single decision tree or other machine learning models. \n",
    "#Bagging can also be combined with other techniques such as boosting or feature selection to further improve the performance of the credit risk models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
